# Image Denoising Autoencoder

This project implements an image denoising system using a convolutional autoencoder in Python, utilizing TensorFlow and Keras. The model is trained on noisy versions of the MNIST dataset, learns to remove noise, and is capable of denoising an external noisy image. The primary objective is to demonstrate how convolutional autoencoders can be used for image noise reduction.

## Project Overview

Image denoising is a common pre-processing step in computer vision tasks to enhance image quality by removing random noise. This project uses a convolutional autoencoder, which is trained to clean noisy images by learning the underlying structure of the data.

### Key Features:
- Training on the **MNIST** dataset with added Gaussian noise.
- Testing the model's performance on both MNIST test images and external noisy images.
- The model removes noise and reconstructs cleaner images.

## Repository Contents

This repository includes the following:
- `image_denoising_autoencoder.ipynb`: The Jupyter Notebook containing the full code for the project, including data preparation, model construction, training, and testing.

### Directory structure:
```bash
image-denoising-autoencoder/
│
├── image_denoising_autoencoder.ipynb  # Main project notebook
├── README.md                          # Project description
```

### Required Packages:
- `numpy`
- `matplotlib`
- `tensorflow`
- `keras`
- `scikit-image`

## Model Architecture

The model is a convolutional autoencoder, which consists of two parts:
- **Encoder**: Compresses the input noisy image into a latent space representation.
- **Decoder**: Reconstructs the clean image from the latent representation.

The architecture includes:
- Convolutional layers with `ReLU` activations for feature extraction.
- MaxPooling layers for downsampling.
- Upsampling layers to reconstruct the original image size.

### Model Summary:

| Layer             | Output Shape       | Parameters  |
|-------------------|--------------------|-------------|
| Conv2D            | (28, 28, 32)       | 320         |
| MaxPooling2D      | (14, 14, 32)       | 0           |
| Conv2D            | (14, 14, 64)       | 18,496      |
| MaxPooling2D      | (7, 7, 64)         | 0           |
| Conv2D            | (7, 7, 64)         | 36,928      |
| UpSampling2D      | (14, 14, 64)       | 0           |
| Conv2D            | (14, 14, 32)       | 18,464      |
| UpSampling2D      | (28, 28, 32)       | 0           |
| Conv2D            | (28, 28, 1)        | 289         |

## Dataset

The project uses the **MNIST** dataset of handwritten digits for training and testing. The dataset contains 60,000 training images and 10,000 test images. Noise is added to the dataset to simulate real-world noisy images.

The noisy images are generated by adding Gaussian noise with the following configuration:
- `noise_factor = 0.5`

## Running the Project

1. **Clone the repository**:
   ```bash
   git clone https://github.com/ahmdmohamedd/image-denoising-autoencoder.git
   cd image-denoising-autoencoder
   ```

2. **Run the Jupyter Notebook**:
   Launch Jupyter Notebook and open `image_denoising_autoencoder.ipynb`. Run all cells to train the autoencoder and visualize the results.

   ```bash
   jupyter notebook
   ```

3. **Testing on an external image**:
   - Place your external image in the `external_images/` directory.
   - Modify the image path in the notebook (last cell) to load your image and see the denoising result.

## Results

The model is trained on noisy MNIST images for 10 epochs. After training, it is able to effectively reduce noise from test images, producing clean and clear results.

## External Testing

The system also supports denoising of external images. Simply place an image in grayscale format (preferably of size 28x28) or let the system resize and process it.

## Future Improvements

Potential enhancements include:
- Using more complex datasets like CIFAR-10 for higher-resolution denoising.
- Adding more sophisticated noise types such as speckle or salt-and-pepper noise.
- Experimenting with deeper autoencoder architectures for better performance on complex datasets.
